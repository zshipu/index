# 进入 — 深入设计：图像预处理 与 网格分离（去网格、去红网格、倾斜/透视校正、背景建模）

下面是**UltraThink 深度方案**，目标把心电纸图（手机拍照 / 扫描 / PDF）变成对后续 trace-extraction 最友好的图像：网格被抑制或移除、曲线保全、几何失真纠正、亮度/颜色干扰最小化。包含：原理、逐步算法、可运行 PoC 代码、参数解释、验证指标、失败降级策略、工程级优化与部署建议。你会得到一个可以直接落地到工程里的实现蓝图。

---

# 假设与前提

* 输入：彩色或灰度图（任意方向、含透视、可能有手写注记/遮挡）。
* 常见纸速/幅度注记：可能存在（25 mm/s, 10 mm/mV），但不强制要求 OCR 成功。
* 目标：最大化保留 ECG trace（可能为黑/红/蓝）同时清除/弱化网格线与背景噪声，输出：`preproc_image`（用于后续分割）与 `grid_meta`（像素/小格/mm 等可用于标定的元信息，若能检测到）。

---

# 输出（接口约定）

函数/服务输出：

* `preproc_img`：三通道或单通道图像，网格弱化，trace清晰；
* `mask_trace_candidate`：二值掩码，高概率为 ECG trace 像素（为后续 skeletonization 做准备）；
* `grid_info`：`{pixels_per_mm, small_box_px, big_box_px, paper_speed_mm_s_guess, voltage_mm_per_mV_guess, calibration_source}` 或 `None`（若未检测到）；
* `warnings`：检测到的异常（透视严重、无网格、低分辨率等）。

---

# 高层流水线（步骤概览）

1. Input sanity checks（分辨率、文件类型）
2. Color normalization and channel separation（红色/黑色曲线判别）
3. Geometric correction：deskew → perspective rectify (per-panel)
4. Grid detection：frequency / projection / Hough / autocorrelation → 得到 `pixels_per_mm`
5. Grid removal：保护 trace 的去网格（优先使用 mask-protected morphological / inpainting / frequency domain）
6. Background normalization（CLAHE / local background subtraction）
7. Trace candidate mask extraction（adaptive thresh + morphological + connected component）
8. Output + QC metrics

接下来逐步拆解并给出代码实现建议与参数。

---

# 1 — 输入校验（必须）

检查：

* 分辨率（建议最小短边 > 800 px；最佳 > 1500 px）
* 色深与通道（RGB 优先）
* 文件大小与解码成功

降级策略：若低分辨率返回 `warnings` 并以低置信度进行处理；提示用户重拍（含格尺/更近距离）。

---

# 2 — 颜色归一化与通道分离（关键：区分曲线与网格）

思路：

* ECG trace 常为 **深色**（黑/红/蓝），网格通常为浅粉/浅红/浅褐或淡灰。
* 使用 HSV / LAB 空间，结合色差：例如 `R - (G+B)/2` 可增强红线；`L channel` (Lab) 可用于亮度差异。
* 若 trace 与网格颜色接近，则使用局部对比增强 + texture（频域）判别。

代码思路（核心）：

```python
# 假设 img is BGR (OpenCV)
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
r_channel = img[:,:,2].astype(int)
g_channel = img[:,:,1].astype(int)
b_channel = img[:,:,0].astype(int)
red_score = r_channel - ((g_channel + b_channel)//2)   # positive → red-ish
dark_score = 255 - cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # dark pixels
```

策略：

* compute `trace_hint = max(red_score_norm, dark_score_norm)` → prioritizes red or dark traces
* use Otsu / adaptive threshold on `trace_hint` to get initial candidate pixels

参数建议：

* `red_score_norm` clip to [0,255], apply Gaussian blur σ=1–2.
* Threshold for red detection: > 20–30 (depends on camera)

---

# 3 — 倾斜/旋转校正（deskew）

目标：将图像中网格线或文本的主方向水平化，减少后续误判。

方法：

* Compute edges: `Canny(gray)` → `HoughLinesP` → collect angles of near-horizontal lines → median angle → rotate by `-angle`.
* Alternate: use FFT-based projection profile to estimate skew.

参数：

* Canny thresholds: (50,150) or adaptive by median intensity.
* Hough: `rho=1, theta=np.pi/180, threshold= max(100, img_width//20)`。

代码概念：

```python
edges = cv2.Canny(gray, 50, 150)
lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=threshold, minLineLength=img_width//4, maxLineGap=20)
angles = [atan2(y2-y1, x2-x1) for each line if abs(angle) < 15deg]
angle_deg = median(angles) * 180/pi
rotated = rotate_image(img, -angle_deg)
```

注意：

* 仅在检测到足够长的线时才旋转，否则跳过（避免曲线主方向误判）。
* For per-panel images (12-lead grid in blocks), do global deskew first, then per-panel refine.

---

# 4 — 透视校正（Perspective rectify）

当图片为手机拍照并出现梯形扭曲时，需做单应性变换（homography）恢复正投影，步骤：

1. Detect grid intersections (网格交点) 或四角（面板边缘）：

   * Use morphological to detect strong vertical/horizontal lines, then their intersections.
   * Cluster intersection points into rectangles (RANSAC).
2. For each detected panel rectangle, compute perspective transform to canonical rectangle.

实现要点：

* Use Hough to detect vertical/horizontal lines; compute intersections.
* If intersections sparse, use edge detection + contour detection for rectangular boxes (panels).
* Accept transform only if rectangle aspect ratio close to expected (e.g., width/height ~ 4:3) or if area > threshold.

代码 snippet:

```python
# Given src_pts (4 corners), dst_pts = np.array([[0,0],[W,0],[W,H],[0,H]])
M = cv2.getPerspectiveTransform(src_pts.astype(np.float32), dst_pts.astype(np.float32))
warped = cv2.warpPerspective(img, M, (W, H))
```

Parameter choices:

* W, H choose based on median panel size * scale factor (e.g., 1.0).

Fallback:

* If can't find reliable corners, continue without perspective rectification but mark `warnings`.

---

# 5 — 网格检测（pixels_per_mm 估算：核心）

多重方法并用（稳健性）：

### 方法 A — 投影 & 峰值法（简单、快速）

* Convert to grayscale, apply low-pass blur.
* Compute vertical projection (mean intensity per column) and horizontal projection (mean per row).
* Find peaks in projection → spacing between adjacent peaks → small-box px estimation = median(diff(peaks_small)).
* Distinguish small-box vs big-box by k-means on distances (clusters around `d` and `5d`).

优点：轻量，适用网格清晰图像。

### 方法 B — Autocorrelation / FFT（对周期性网格稳定）

* Compute 1D autocorrelation of projection signal; find dominant periodicity `p` → pixels_per_small_box ~ p.
* FFT: magnitude peaks correspond to grid frequency.

### 方法 C — Hough Line Clustering

* Detect many vertical lines with HoughLinesP; sort by x coordinate → compute diffs → median.

Combine strategies:

* Compute estimates from A/B/C, compute robust median, check consistency (std/mean ratio). If consistent → accept `pixels_per_mm = estimate`.

Equations (remember digit-by-digit for arithmetic if needed):

* paper_speed (s/mm) default 0.04 (for 25 mm/s): `time_per_pixel = paper_speed / pixels_per_mm = 0.04 / g`
* mV per pixel: `0.1 / g` for 10 mm/mV standard.

QC:

* Validate `small_box_px` >= 3 px; otherwise resolution too low.

---

# 6 — 网格去除（核心）：**保护 trace** 的去网格策略（优先级）

目标：移除网格线同时**不破坏**waveform像素。常见误区是直接 morphological open 会破坏细小尖峰（QRS），因此采用“保护掩码 + 去格”方法。

总体策略（工程级，推荐顺序）：

### STEP A — 生成 Trace Protection Mask（首要）

* From color separation and local contrast, generate a conservative binary mask `M_trace` of pixels likely to be ECG trace (high recall desired).

  * Use `trace_hint` (从第2步） + morphological closing to connect broken trace segments.
  * Use connected component filtering: keep components with length > Lmin or aspect ratio consistent with line (thin & long).
* Expand `M_trace` by a small radius (dilate with kernel radius = 1~2 px) to protect neighborhood.

### STEP B — Detect Grid Lines (precise)

* From step 5, we have grid spacing `s_px`. Use matched filters:

  * Create vertical line kernel `k_v` of width 1 and height `s_px` or morphological structuring element size ~ `int(s_px/2)`.
  * Use morphological opening with `k_v` to capture vertical lines; similarly for horizontal `k_h`.
* OR use Hough to get exact line pixels.

### STEP C — Remove Grid While Preserving Trace

Options (ordered by robustness):

**C1 — Masked Inpainting (recommended)**

* Create `grid_mask` where grid lines present AND NOT `M_trace`.
* Use `cv2.inpaint` on original image with `grid_mask` to reconstruct background using neighboring pixels, leaving trace intact by excluding `M_trace` from mask.
* Inpainting method: `INPAINT_TELEA` or `INPAINT_NS` (telea usually faster, ns may be smoother).

**C2 — Frequency-domain notch (FFT) + reconstruction**

* Transform large blocks to frequency domain, suppress frequencies corresponding to grid periodicity, inverse FFT.
* Risk: may blur trace if frequencies overlap.

**C3 — Morphological remove + reconstruction**

* Remove grid lines by morphological open/closing on whole image **but** re-insert `M_trace` pixels from original to avoid losing trace.
* Steps:

  1. `img_grid_removed = morphological_open(img_gray, kernel_grid)`
  2. `img_restored = where(M_trace, original_img, img_grid_removed)`

**C4 — Learned inpainting (advanced)**

* Use small U-Net trained to inpaint grid segments given masked input. Good when grid color close to trace.

Parameters for inpainting:

* Kernel radius for dilation of trace mask: `r_protect = max(1, round(s_px*0.02))` (2% of small grid).
* For morphological operations: `kernel_size = max(3, int(s_px*0.3))` for opening on grid detection.

Quality check after removal:

* Compute local contrast ratio between protected trace pixels vs immediate background; ensure trace intensity preserved within tolerance (e.g., peak intensity drop < 10%).

---

# 7 — 背景归一化与对比度增强

在去网格后剩余亮度不均或纸张阴影需要处理：

* Use CLAHE on L channel (Lab) with `clipLimit=2.0`, `tileGridSize=(8,8)`.
* Or perform local background estimation: apply large-kernel Gaussian blur `G_sigma = max(img_dim/100, 15)` to get background `B(x,y)`, then `I_norm = (I - B) + mean_background_level`.
* Normalize contrast to span useful intensity range, avoid clipping.

---

# 8 — Trace candidate final mask（供后续 skeleton）

* After grid removal and contrast norm, threshold `trace_hint` again (adaptive thresholding, blockSize ~ `int(s_px*3)`).
* Morphological thinning / skeletonization: `cv2.ximgproc.thinning` or `skimage.morphology.skeletonize`.
* Post-filter components: remove tiny islands < area threshold (e.g., area < `s_px*3`).

Output `mask_trace_candidate` (binary) and `annotated_img` showing overlay.

---

# 9 — QC 指标（自动化判断是否成功）

Compute and return:

* `pixels_per_mm` (or None)
* `trace_coverage_ratio` = (# trace pixels / total pixels in ROI)
* `trace_continuity_score` = fraction of columns having at least 1 trace pixel (goal > 0.85)
* `skeleton_branching_rate` = #branch_points / length (too high → noisy)
* `peak_intensity_preservation` = median(original_trace_intensity / processed_trace_intensity) (target ~1.0 ± 0.15)
* `warnings` if any: low resolution, no grid detected, low trace continuity, heavy perspective distortion.

---

# 10 — 可运行 PoC：完整 Python 实现（OpenCV + skimage 组合）

下面给出一个**端到端 PoC**脚本（去网格、deskew、perspective attempt、trace protection + inpaint）。可直接放入工程做快速试验。注：为简洁省去部分异常处理，工程化时请包裹异常并记录 telemetry。

```python
# ecg_preproc_poc.py
import cv2
import numpy as np
from skimage.morphology import skeletonize
from scipy.signal import find_peaks

def read_img(path):
    img = cv2.imread(path, cv2.IMREAD_COLOR)
    return img

def to_gray(img):
    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

def quick_deskew(gray):
    edges = cv2.Canny(gray, 50, 150)
    lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=200, minLineLength=gray.shape[1]//4, maxLineGap=20)
    if lines is None:
        return gray, 0.0
    angles = []
    for x1,y1,x2,y2 in lines[:,0]:
        ang = np.degrees(np.arctan2(y2-y1, x2-x1))
        if abs(ang) < 45:
            angles.append(ang)
    if len(angles)==0:
        return gray, 0.0
    angle = np.median(angles)
    M = cv2.getRotationMatrix2D((gray.shape[1]/2, gray.shape[0]/2), angle, 1.0)
    rotated = cv2.warpAffine(gray, M, (gray.shape[1], gray.shape[0]), flags=cv2.INTER_LINEAR)
    return rotated, angle

def estimate_grid_spacing(gray):
    # vertical projection
    proj = np.mean(255 - gray, axis=0)  # dark lines show as peaks
    # smooth
    kernel = np.ones(5)/5
    proj_s = np.convolve(proj, kernel, mode='same')
    # find peaks (they correspond to vertical grid lines)
    peaks, _ = find_peaks(proj_s, distance=5, height=np.percentile(proj_s,60))
    if len(peaks) < 4:
        return None
    diffs = np.diff(peaks)
    small_box = int(np.median(diffs))
    return small_box

def extract_trace_hint(img):
    # img: BGR
    b,g,r = cv2.split(img)
    red_score = cv2.subtract(r, cv2.add(g,b)//2)
    gray = to_gray(img)
    dark_score = cv2.subtract(255, gray)
    # normalize
    rs = cv2.normalize(red_score, None, 0, 255, cv2.NORM_MINMAX)
    ds = cv2.normalize(dark_score, None, 0, 255, cv2.NORM_MINMAX)
    trace_hint = cv2.max(rs, ds)
    trace_hint = cv2.GaussianBlur(trace_hint, (3,3), 0)
    return trace_hint

def get_trace_protection_mask(trace_hint, small_box_px):
    # adaptive threshold depending on small_box_px
    block = max(11, int(small_box_px*3)|1)  # ensure odd
    th = cv2.adaptiveThreshold(trace_hint, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                               cv2.THRESH_BINARY, block, -10)
    # morphological close to connect broken lines
    k = cv2.getStructuringElement(cv2.MORPH_RECT, (1, max(1, small_box_px//6)))
    closed = cv2.morphologyEx(th, cv2.MORPH_CLOSE, k)
    # remove small components
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(closed, connectivity=8)
    mask = np.zeros_like(closed)
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        w = stats[i, cv2.CC_STAT_WIDTH]
        h = stats[i, cv2.CC_STAT_HEIGHT]
        if area > max(20, small_box_px*2) or max(w,h) > small_box_px:  # heuristics
            mask[labels==i] = 255
    # dilate small amount to protect neighbors
    dil_r = max(1, int(round(small_box_px*0.02)))
    mask = cv2.dilate(mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(dil_r*2+1,dil_r*2+1)))
    return mask

def detect_grid_mask(gray, small_box_px):
    # detect vertical and horizontal grid lines via morphological operations
    img_bin = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 51, 10)
    # vertical
    vert_k = cv2.getStructuringElement(cv2.MORPH_RECT, (1, max(3, int(small_box_px*0.5))))
    vert = cv2.morphologyEx(img_bin, cv2.MORPH_OPEN, vert_k)
    # horizontal
    hor_k = cv2.getStructuringElement(cv2.MORPH_RECT, (max(3, int(small_box_px*0.5)), 1))
    hor = cv2.morphologyEx(img_bin, cv2.MORPH_OPEN, hor_k)
    grid_mask = cv2.bitwise_or(vert, hor)
    # optionally refine via thinning
    return grid_mask

def masked_inpaint_color(img_bgr, grid_mask, protect_mask):
    # inpaint only where grid_mask == 255 and protect_mask == 0
    mask = cv2.bitwise_and(grid_mask, cv2.bitwise_not(protect_mask))
    # convert to 8u single channel mask
    mask8 = (mask>0).astype('uint8')*255
    # inpaint on grayscale then blend? OpenCV inpaint supports 3-channel
    inpainted = cv2.inpaint(img_bgr, mask8, 3, cv2.INPAINT_TELEA)
    return inpainted

def final_trace_mask_from_processed(processed_img, small_box_px):
    gray = to_gray(processed_img)
    # adaptive threshold with smaller block to capture fine line
    block = max(9, int(small_box_px*2)|1)
    th = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, block, 5)
    # morphology to remove noise
    k = cv2.getStructuringElement(cv2.MORPH_RECT, (1, max(1, small_box_px//6)))
    cleaned = cv2.morphologyEx(th, cv2.MORPH_OPEN, k)
    # skeletonize
    skeleton = skeletonize((cleaned>0).astype(np.uint8)).astype(np.uint8)*255
    return skeleton

def preproc_ecg_image(path):
    img = read_img(path)
    gray0 = to_gray(img)
    # deskew
    gray_ds, angle = quick_deskew(gray0)
    # attempt to estimate grid spacing
    small_box = estimate_grid_spacing(gray_ds)
    if small_box is None:
        # if cannot find, fallback guess based on image resolution
        small_box = max(4, int(min(img.shape[:2]) / 200))
    # color-based trace hint
    trace_hint = extract_trace_hint(img)
    protect_mask = get_trace_protection_mask(trace_hint, small_box)
    grid_mask = detect_grid_mask(gray_ds, small_box)
    inpainted = masked_inpaint_color(img, grid_mask, protect_mask)
    # further normalize contrast
    lab = cv2.cvtColor(inpainted, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    l2 = clahe.apply(l)
    lab2 = cv2.merge([l2,a,b])
    img_norm = cv2.cvtColor(lab2, cv2.COLOR_LAB2BGR)
    # final mask
    skeleton = final_trace_mask_from_processed(img_norm, small_box)
    # assemble outputs
    grid_info = {'small_box_px': int(small_box), 'pixels_per_mm_est': float(small_box)}  # interpret small_box as px per mm
    return {'preproc_img': img_norm, 'trace_mask': skeleton, 'grid_info': grid_info, 'warnings': []}

if __name__ == "__main__":
    import sys
    path = sys.argv[1]
    out = preproc_ecg_image(path)
    cv2.imwrite('preproc.png', out['preproc_img'])
    cv2.imwrite('mask.png', out['trace_mask'])
    print("grid_info:", out['grid_info'])
```

**说明**：PoC 中 `small_box` 被粗糙估算并直接当作 px/mm；工程上你要用更稳健的 grid spacing estimator（详见上文方法A/B/C 组合），并对图像包含多个 panels 做 per-panel grid estimation。

---

# 11 — 参数解释与调优建议（逐项）

* `Canny thresholds (50,150)`: 对于手机照片一般合适；若低光或高噪声，使用 `median` 自动调参： `low = 0.66*median`, `high = 1.33*median`.
* `Hough minLineLength = width/4`: 找到足够长的网格线或图片边缘。
* `small_box` acceptance threshold: require `small_box >= 3 px` otherwise resolution too low for meaningful mV mapping.
* `AdaptiveThreshold blockSize`: choose proportional to `small_box` to avoid mixing grid frequency.
* `Inpaint radius = 3`: adjust if grid is thick; larger radii may blur trace edges.

---

# 12 — 验证与测试（构建 ground-truth）

必做：

* 收集 **paired dataset**：原始数字 ECG ↔ 同次拍照/打印图像（至少 500–2000 对，覆盖 1/12/25/50 mm/s, different cameras, lighting, annotations）。
* 指标：

  * `trace_continuity_score` (目标 > 0.9 on good photos)
  * `R-peak localization error` after full pipeline and extraction (< 10–20 ms ideal)
  * `mV RMSE` between extracted and ground-truth (< 0.05 mV target depending on resolution)
  * `false_mask_ratio` (mask pixels not part of trace)
* Unit tests for corner cases: heavy shadow, rotated >15°, partial crop, hand-drawn circles, multi-color traces.

Augmentation for ML-based components:

* Random exposure, motion blur, jpeg compression, watermarking, partial occlusion.

---

# 13 — Edge cases与降级策略（必须实现）

* **无网格或被遮挡**：Return `grid_info = None` and produce pixel-scale trace; prompt user to provide paper speed or upload raw signal.
* **Low-res**: if `short_side < 600 px` → flag low-res and reduce expectations; still produce mask but mark low confidence.
* **Multiple overlapping curves (color overlapped)**: attempt color separation; if fail, route to manual review.
* **Strong handwriting or stamps overlapping trace**: detect by connected components of large area and non-linearity; mask them and inpaint, but mark artifact.

---

# 14 — 性能与工程化（部署注意）

* CPU path: OpenCV operations + morphological are CPU-friendly; for throughput (1000 imgs/day) a few machines suffice.
* GPU path: if using U-Net segmentation/inpainting, containerize with CUDA + Triton/ONNXRuntime.
* Pipeline pattern: **preprocess service** (sync lightweight deskew + quick grid estimation) → **heavy worker** (inpaint + segmentation) async via queue (RabbitMQ/Kafka). But present immediate "prelim result" to user UI from fast path.
* Caching: store `preproc_img` and `grid_info`; if user uploads same image again, return cached.
* Observability: emit metrics: `avg_small_box_px`, `% no-grid`, `avg_trace_continuity`, latency P95.

---

# 15 — Integration API (suggestion)

* `POST /api/ecg/image/preprocess` (multipart/form-data: image, optional metadata {paper_speed, scale_mm_mV})
  Response: `{report_id, status, grid_info, preproc_img_url, trace_mask_url, qc_scores, warnings}`

* `GET /api/ecg/image/preprocess/{report_id}` returns stored outputs.

Design: keep preproc deterministic and versioned (`pipeline_version`), store pipeline config in DB for reproducibility.

---

# 16 — 进一步提高（roadmap）

1. **Train segmentation U-Net** to replace heuristic mask → much better in difficult backgrounds. Use synthetic dataset pipeline described earlier.
2. **Learned inpainting** for grid removal (small UNet trained on pairs: original with grid → clean).
3. **Blind calibration module**: estimate `pixels_per_mm` via learning when grid not present by recognizing waveform frequency patterns & typical ECG heartbeat spacing.
4. **Mobile on-device preproc**: lightweight OpenCV-only pipeline embedded in mobile app for instant feedback to user (reduce upload failures).

---

# 17 — 最后：快速检查清单（ready-to-run）

* [ ] Implement PoC and run on 50 real-world images.
* [ ] Build paired dataset (image + raw ECG) for 500 cases.
* [ ] Add per-panel perspective rectify for 12-lead sheets.
* [ ] Replace heuristic protection mask with trained segmentation if >5% failure rate.
* [ ] Integrate QC and user messaging for low-confidence outputs.

---

