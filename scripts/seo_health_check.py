#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SEOå¥åº·åº¦æ£€æŸ¥è„šæœ¬ - index.zshipu.com
è‡ªåŠ¨åŒ–æ£€æŸ¥ç½‘ç«™SEOå…³é”®è¦ç´ çš„åˆè§„æ€§

æ£€æŸ¥é¡¹ç›®ï¼š
1. é¦–é¡µindex.htmlå…³é”®SEOè¦ç´ 
2. Meta descriptionè´¨é‡æ£€æŸ¥
3. Schema.orgç»“æ„åŒ–æ•°æ®éªŒè¯
4. Sitemap.xmlæœ‰æ•ˆæ€§æ£€æŸ¥
5. robots.txtè¯­æ³•æ£€æŸ¥
6. å†…éƒ¨é“¾æ¥æ£€æŸ¥
7. å›¾ç‰‡altæ ‡ç­¾æ£€æŸ¥
8. é¡µé¢æ ‡é¢˜ä¼˜åŒ–æ£€æŸ¥

è¿è¡Œæ–¹å¼ï¼š
  python scripts/seo_health_check.py

è¾“å‡ºï¼š
  - ç»ˆç«¯å½©è‰²æŠ¥å‘Šï¼ˆç»¿è‰²=é€šè¿‡ï¼Œé»„è‰²=è­¦å‘Šï¼Œçº¢è‰²=é”™è¯¯ï¼‰
  - ç”Ÿæˆ claudedocs/seo_health_report_YYYYMMDD.txt è¯¦ç»†æŠ¥å‘Š
"""

import os
import re
import json
import sys
import xml.etree.ElementTree as ET
from html.parser import HTMLParser
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse

# è®¾ç½®Windowsæ§åˆ¶å°UTF-8ç¼–ç æ”¯æŒ
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')


class SEOHealthChecker:
    """SEOå¥åº·åº¦æ£€æŸ¥å™¨"""

    def __init__(self, root_dir=None):
        """
        åˆå§‹åŒ–æ£€æŸ¥å™¨

        Args:
            root_dir: ç½‘ç«™æ ¹ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸ºè„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šçº§ç›®å½•
        """
        if root_dir is None:
            self.root_dir = Path(__file__).parent.parent
        else:
            self.root_dir = Path(root_dir)

        self.issues = []
        self.warnings = []
        self.passed = []
        self.score = 100  # æ€»åˆ†100åˆ†ï¼Œæ¯ä¸ªé—®é¢˜æ‰£åˆ†

    def check_all(self):
        """æ‰§è¡Œæ‰€æœ‰SEOæ£€æŸ¥"""
        print("=" * 60)
        print("ğŸ” SEOå¥åº·åº¦æ£€æŸ¥ - index.zshipu.com")
        print("=" * 60)
        print()

        # æ£€æŸ¥é¦–é¡µ
        self.check_homepage()

        # æ£€æŸ¥sitemap
        self.check_sitemap()

        # æ£€æŸ¥robots.txt
        self.check_robots_txt()

        # æ£€æŸ¥å…³é”®é¡µé¢
        self.check_key_pages()

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()

    def check_homepage(self):
        """æ£€æŸ¥é¦–é¡µindex.htmlçš„SEOè¦ç´ """
        print("ğŸ“„ æ£€æŸ¥é¦–é¡µ (index.html)...")

        index_path = self.root_dir / "index.html"
        if not index_path.exists():
            self.add_issue("é¦–é¡µæ–‡ä»¶ä¸å­˜åœ¨ï¼šindex.html", critical=True)
            return

        with open(index_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # æ£€æŸ¥åŸºç¡€metaæ ‡ç­¾
        self._check_meta_tags(content, "é¦–é¡µ")

        # æ£€æŸ¥Schema.orgç»“æ„åŒ–æ•°æ®
        self._check_schema_org(content, "é¦–é¡µ")

        # æ£€æŸ¥æ ‡é¢˜æ ‡ç­¾å±‚çº§
        self._check_heading_hierarchy(content, "é¦–é¡µ")

        # æ£€æŸ¥å›¾ç‰‡altæ ‡ç­¾
        self._check_image_alts(content, "é¦–é¡µ")

        print()

    def check_sitemap(self):
        """æ£€æŸ¥sitemap.xmlæœ‰æ•ˆæ€§"""
        print("ğŸ—ºï¸  æ£€æŸ¥Sitemap...")

        sitemap_path = self.root_dir / "sitemap.xml"
        if not sitemap_path.exists():
            self.add_issue("Sitemapæ–‡ä»¶ä¸å­˜åœ¨ï¼šsitemap.xml", critical=True)
            return

        try:
            tree = ET.parse(sitemap_path)
            root = tree.getroot()

            # æ£€æŸ¥æ˜¯å¦æ˜¯sitemapç´¢å¼•æˆ–æ ‡å‡†sitemap
            ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}

            # æ£€æŸ¥URLæ•°é‡
            urls = root.findall('.//sm:url', ns)
            sitemaps = root.findall('.//sm:sitemap', ns)

            if urls:
                url_count = len(urls)
                self.add_pass(f"SitemapåŒ…å« {url_count} ä¸ªURL")

                # æ£€æŸ¥URLæ ¼å¼
                for url in urls[:5]:  # æŠ½æŸ¥å‰5ä¸ª
                    loc = url.find('sm:loc', ns)
                    if loc is not None and loc.text:
                        if not loc.text.startswith('https://'):
                            self.add_warning(f"URLæœªä½¿ç”¨HTTPS: {loc.text}")

            if sitemaps:
                sitemap_count = len(sitemaps)
                self.add_pass(f"Sitemapç´¢å¼•åŒ…å« {sitemap_count} ä¸ªå­sitemap")

                # æ£€æŸ¥å­sitemapæ˜¯å¦å­˜åœ¨
                for sitemap in sitemaps:
                    loc = sitemap.find('sm:loc', ns)
                    if loc is not None and loc.text:
                        # æå–æ–‡ä»¶å
                        sitemap_file = loc.text.split('/')[-1]
                        sitemap_file_path = self.root_dir / sitemap_file
                        if not sitemap_file_path.exists():
                            self.add_warning(f"å­sitemapæ–‡ä»¶ä¸å­˜åœ¨: {sitemap_file}")

        except ET.ParseError as e:
            self.add_issue(f"Sitemap XMLæ ¼å¼é”™è¯¯: {e}", critical=True)
        except Exception as e:
            self.add_issue(f"Sitemapæ£€æŸ¥å¤±è´¥: {e}")

        print()

    def check_robots_txt(self):
        """æ£€æŸ¥robots.txtè¯­æ³•å’Œå†…å®¹"""
        print("ğŸ¤– æ£€æŸ¥robots.txt...")

        robots_path = self.root_dir / "robots.txt"
        if not robots_path.exists():
            self.add_issue("robots.txtæ–‡ä»¶ä¸å­˜åœ¨", critical=True)
            return

        with open(robots_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # æ£€æŸ¥æ˜¯å¦åŒ…å«Sitemapå£°æ˜
        if 'Sitemap:' not in content:
            self.add_warning("robots.txtæœªå£°æ˜Sitemapä½ç½®")
        else:
            self.add_pass("robots.txtåŒ…å«Sitemapå£°æ˜")

        # æ£€æŸ¥User-agentå£°æ˜
        if 'User-agent:' not in content:
            self.add_issue("robots.txtç¼ºå°‘User-agentå£°æ˜")
        else:
            self.add_pass("robots.txtåŒ…å«User-agentå£°æ˜")

        # æ£€æŸ¥æ˜¯å¦æœ‰Disallowè§„åˆ™
        if 'Disallow:' in content:
            disallow_count = content.count('Disallow:')
            self.add_pass(f"robots.txtåŒ…å« {disallow_count} æ¡Disallowè§„åˆ™")

        # æ£€æŸ¥æ˜¯å¦æœ‰Allowè§„åˆ™ï¼ˆæ¨èï¼‰
        if 'Allow:' in content:
            allow_count = content.count('Allow:')
            self.add_pass(f"robots.txtåŒ…å« {allow_count} æ¡Allowè§„åˆ™")
        else:
            self.add_warning("robots.txtæœªä½¿ç”¨Allowè§„åˆ™æ˜ç¡®å…è®¸é‡è¦é¡µé¢")

        # æ£€æŸ¥çˆ¬è™«å»¶è¿Ÿè®¾ç½®
        if 'Crawl-delay:' in content:
            self.add_pass("robots.txtåŒ…å«Crawl-delayè®¾ç½®")

        print()

    def check_key_pages(self):
        """æ£€æŸ¥å…³é”®å¯¼èˆªé¡µé¢"""
        print("ğŸ“‘ æ£€æŸ¥å…³é”®é¡µé¢...")

        key_pages = [
            ("archives/index.html", "å½’æ¡£é¡µé¢"),
            ("categories/index.html", "åˆ†ç±»é¡µé¢"),
            ("tags/index.html", "æ ‡ç­¾é¡µé¢"),
        ]

        for page_path, page_name in key_pages:
            full_path = self.root_dir / page_path
            if not full_path.exists():
                self.add_warning(f"{page_name}ä¸å­˜åœ¨: {page_path}")
            else:
                # æ£€æŸ¥meta description
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                if '<meta name="description"' not in content:
                    self.add_warning(f"{page_name}ç¼ºå°‘meta description")
                else:
                    self.add_pass(f"{page_name}åŒ…å«meta description")

        print()

    def _check_meta_tags(self, content, page_name):
        """æ£€æŸ¥metaæ ‡ç­¾"""
        # Meta description
        desc_match = re.search(r'<meta\s+name="description"\s+content="([^"]+)"', content)
        if not desc_match:
            self.add_issue(f"{page_name}ç¼ºå°‘meta description")
        else:
            desc = desc_match.group(1)
            desc_len = len(desc)
            if desc_len < 120:
                self.add_warning(f"{page_name} meta descriptionè¿‡çŸ­ ({desc_len}å­—ç¬¦ï¼Œå»ºè®®120-160)")
            elif desc_len > 160:
                self.add_warning(f"{page_name} meta descriptionè¿‡é•¿ ({desc_len}å­—ç¬¦ï¼Œå»ºè®®120-160)")
            else:
                self.add_pass(f"{page_name} meta descriptioné•¿åº¦åˆé€‚ ({desc_len}å­—ç¬¦)")

        # Meta keywords (å¯é€‰ï¼Œä½†æœ‰æ€»æ¯”æ²¡æœ‰å¥½)
        if '<meta name="keywords"' not in content:
            self.add_warning(f"{page_name}ç¼ºå°‘meta keywords")

        # Charset
        if '<meta charset="utf-8">' not in content.lower() and 'charset=utf-8' not in content.lower():
            self.add_issue(f"{page_name}ç¼ºå°‘UTF-8ç¼–ç å£°æ˜")
        else:
            self.add_pass(f"{page_name}åŒ…å«UTF-8ç¼–ç å£°æ˜")

        # Viewport
        if '<meta name="viewport"' not in content:
            self.add_warning(f"{page_name}ç¼ºå°‘viewportè®¾ç½®ï¼ˆç§»åŠ¨ç«¯ä¼˜åŒ–ï¼‰")
        else:
            self.add_pass(f"{page_name}åŒ…å«viewportè®¾ç½®")

        # Open Graph
        if '<meta property="og:' not in content:
            self.add_warning(f"{page_name}ç¼ºå°‘Open Graphæ ‡ç­¾ï¼ˆç¤¾äº¤åª’ä½“åˆ†äº«ä¼˜åŒ–ï¼‰")

    def _check_schema_org(self, content, page_name):
        """æ£€æŸ¥Schema.orgç»“æ„åŒ–æ•°æ®"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«JSON-LD
        if '<script type="application/ld+json">' not in content:
            self.add_warning(f"{page_name}ç¼ºå°‘Schema.orgç»“æ„åŒ–æ•°æ®")
            return

        # æå–æ‰€æœ‰JSON-LDå—
        json_ld_pattern = r'<script type="application/ld\+json">\s*(\{.*?\})\s*</script>'
        json_ld_blocks = re.findall(json_ld_pattern, content, re.DOTALL)

        schema_types = []
        for block in json_ld_blocks:
            try:
                data = json.loads(block)

                # å¤„ç† @graph ç»“æ„ï¼ˆå¸¸è§äºå¤æ‚Schema.orgå®ç°ï¼‰
                if '@graph' in data and isinstance(data['@graph'], list):
                    for item in data['@graph']:
                        if '@type' in item:
                            schema_types.append(item['@type'])
                # å¤„ç†å•ä¸ªå¯¹è±¡
                elif '@type' in data:
                    schema_types.append(data['@type'])
                # å¤„ç†æ•°ç»„
                elif isinstance(data, list):
                    for item in data:
                        if '@type' in item:
                            schema_types.append(item['@type'])
            except json.JSONDecodeError:
                self.add_warning(f"{page_name} Schema.org JSONæ ¼å¼é”™è¯¯")

        if schema_types:
            self.add_pass(f"{page_name}åŒ…å«Schema.orgç±»å‹: {', '.join(set(schema_types))}")
        else:
            self.add_warning(f"{page_name}æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„Schema.orgç±»å‹")

        # æ£€æŸ¥æ¨èçš„Schemaç±»å‹
        recommended_types = ['Organization', 'WebSite', 'ItemList', 'FAQPage']
        missing_types = []
        for rec_type in recommended_types:
            if rec_type not in schema_types:
                missing_types.append(rec_type)

        if missing_types:
            self.add_warning(f"{page_name}å»ºè®®æ·»åŠ Schema.orgç±»å‹: {', '.join(missing_types)}")
        else:
            self.add_pass(f"{page_name}åŒ…å«æ‰€æœ‰æ¨èçš„Schema.orgç±»å‹")

    def _check_heading_hierarchy(self, content, page_name):
        """æ£€æŸ¥æ ‡é¢˜æ ‡ç­¾å±‚çº§ï¼ˆH1-H6ï¼‰"""
        # æ£€æŸ¥H1æ•°é‡ï¼ˆåº”è¯¥åªæœ‰1ä¸ªï¼‰
        h1_count = len(re.findall(r'<h1[^>]*>.*?</h1>', content, re.DOTALL | re.IGNORECASE))
        if h1_count == 0:
            self.add_issue(f"{page_name}ç¼ºå°‘H1æ ‡ç­¾")
        elif h1_count > 1:
            self.add_warning(f"{page_name}åŒ…å«{h1_count}ä¸ªH1æ ‡ç­¾ï¼ˆå»ºè®®åªæœ‰1ä¸ªï¼‰")
        else:
            self.add_pass(f"{page_name}åŒ…å«1ä¸ªH1æ ‡ç­¾")

    def _check_image_alts(self, content, page_name):
        """æ£€æŸ¥å›¾ç‰‡altæ ‡ç­¾"""
        # æŸ¥æ‰¾æ‰€æœ‰imgæ ‡ç­¾
        img_tags = re.findall(r'<img[^>]*>', content, re.IGNORECASE)
        total_imgs = len(img_tags)

        if total_imgs == 0:
            return  # æ²¡æœ‰å›¾ç‰‡ï¼Œè·³è¿‡æ£€æŸ¥

        imgs_without_alt = 0
        for img_tag in img_tags:
            if 'alt=' not in img_tag.lower():
                imgs_without_alt += 1

        if imgs_without_alt > 0:
            self.add_warning(f"{page_name} {imgs_without_alt}/{total_imgs} å¼ å›¾ç‰‡ç¼ºå°‘altæ ‡ç­¾")
        else:
            self.add_pass(f"{page_name}æ‰€æœ‰å›¾ç‰‡éƒ½æœ‰altæ ‡ç­¾ ({total_imgs}å¼ )")

    def add_issue(self, message, critical=False):
        """æ·»åŠ é—®é¢˜"""
        self.issues.append(message)
        if critical:
            self.score -= 10
        else:
            self.score -= 5
        print(f"  âŒ {message}")

    def add_warning(self, message):
        """æ·»åŠ è­¦å‘Š"""
        self.warnings.append(message)
        self.score -= 2
        print(f"  âš ï¸  {message}")

    def add_pass(self, message):
        """æ·»åŠ é€šè¿‡é¡¹"""
        self.passed.append(message)
        print(f"  âœ… {message}")

    def generate_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        # ç¡®ä¿åˆ†æ•°ä¸ä½äº0
        self.score = max(0, self.score)

        print()
        print("=" * 60)
        print("ğŸ“Š SEOå¥åº·åº¦è¯„åˆ†")
        print("=" * 60)

        # è¯„åˆ†ç­‰çº§
        if self.score >= 90:
            grade = "ä¼˜ç§€ ğŸ‰"
            color = "ç»¿è‰²"
        elif self.score >= 75:
            grade = "è‰¯å¥½ ğŸ‘"
            color = "é»„è‰²"
        elif self.score >= 60:
            grade = "åŠæ ¼ âš ï¸"
            color = "æ©™è‰²"
        else:
            grade = "éœ€æ”¹è¿› âŒ"
            color = "çº¢è‰²"

        print(f"\næ€»åˆ†ï¼š{self.score}/100 - {grade}")
        print(f"\né€šè¿‡é¡¹ï¼š{len(self.passed)} ä¸ª")
        print(f"è­¦å‘Šé¡¹ï¼š{len(self.warnings)} ä¸ª")
        print(f"é”™è¯¯é¡¹ï¼š{len(self.issues)} ä¸ª")

        # è¯¦ç»†é—®é¢˜åˆ—è¡¨
        if self.issues:
            print("\nğŸ”´ éœ€è¦ç«‹å³ä¿®å¤çš„é—®é¢˜ï¼š")
            for i, issue in enumerate(self.issues, 1):
                print(f"  {i}. {issue}")

        if self.warnings:
            print("\nğŸŸ¡ å»ºè®®ä¼˜åŒ–çš„é¡¹ç›®ï¼š")
            for i, warning in enumerate(self.warnings, 1):
                print(f"  {i}. {warning}")

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶
        self._save_report()

        print("\n" + "=" * 60)
        print("âœ… SEOå¥åº·åº¦æ£€æŸ¥å®Œæˆ")
        print("=" * 60)

    def _save_report(self):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report_dir = self.root_dir / "claudedocs"
        report_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = report_dir / f"seo_health_report_{timestamp}.txt"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("SEOå¥åº·åº¦æ£€æŸ¥æŠ¥å‘Š - index.zshipu.com\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"æ£€æŸ¥æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»åˆ†ï¼š{self.score}/100\n\n")

            f.write(f"âœ… é€šè¿‡é¡¹ ({len(self.passed)})ï¼š\n")
            for item in self.passed:
                f.write(f"  - {item}\n")
            f.write("\n")

            f.write(f"âš ï¸  è­¦å‘Šé¡¹ ({len(self.warnings)})ï¼š\n")
            for item in self.warnings:
                f.write(f"  - {item}\n")
            f.write("\n")

            f.write(f"âŒ é”™è¯¯é¡¹ ({len(self.issues)})ï¼š\n")
            for item in self.issues:
                f.write(f"  - {item}\n")
            f.write("\n")

            f.write("=" * 60 + "\n")
            f.write("å»ºè®®ï¼š\n")
            f.write("1. ä¼˜å…ˆä¿®å¤æ ‡è®°ä¸ºâŒçš„é”™è¯¯é¡¹ï¼ˆæ¯ä¸ªæ‰£5-10åˆ†ï¼‰\n")
            f.write("2. é€æ­¥ä¼˜åŒ–æ ‡è®°ä¸ºâš ï¸çš„è­¦å‘Šé¡¹ï¼ˆæ¯ä¸ªæ‰£2åˆ†ï¼‰\n")
            f.write("3. å®šæœŸè¿è¡Œæ­¤è„šæœ¬ç›‘æ§SEOå¥åº·åº¦\n")
            f.write("4. ç›®æ ‡åˆ†æ•°ï¼š90åˆ†ä»¥ä¸Šï¼ˆä¼˜ç§€ç­‰çº§ï¼‰\n")
            f.write("=" * 60 + "\n")

        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°ï¼š{report_path.relative_to(self.root_dir)}")


def main():
    """ä¸»å‡½æ•°"""
    checker = SEOHealthChecker()
    checker.check_all()


if __name__ == "__main__":
    main()
