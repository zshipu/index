#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†é“ºå…¨ç«™é“¾æ¥ç´¢å¼•ç”Ÿæˆå™¨ - ULTRATHINKå†…é“¾ä¼˜åŒ– + SEO Sitemap
ç”ŸæˆJSONæ ¼å¼çš„å…¨ç«™æ–‡ç« ç´¢å¼•å’ŒXML sitemapï¼Œç”¨äºï¼š
1. Archivesé¡µé¢ç«™ç‚¹åœ°å›¾
2. é¦–é¡µå³ä¾§æ å†…é“¾å¡ç‰‡
3. å…¨ç«™æœç´¢åŠŸèƒ½
4. SEO: å®Œæ•´sitemap.xmlç”Ÿæˆï¼ˆåŒ…å«æ‰€æœ‰3725+ç¯‡æ–‡ç« ï¼‰
"""

import os
import sys
import io
import json
import re
from pathlib import Path
from datetime import datetime
from html.parser import HTMLParser
from collections import defaultdict

# ä¿®å¤Windowsæ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

class TitleExtractor(HTMLParser):
    """HTMLæ ‡é¢˜æå–å™¨"""
    def __init__(self):
        super().__init__()
        self.in_title = False
        self.title = ""

    def handle_starttag(self, tag, attrs):
        if tag == "title":
            self.in_title = True

    def handle_data(self, data):
        if self.in_title:
            self.title += data

    def handle_endtag(self, tag):
        if tag == "title":
            self.in_title = False

def clean_title(title):
    """æ¸…ç†æ ‡é¢˜"""
    # ç§»é™¤å¸¸è§åç¼€
    title = re.sub(r'--çŸ¥è¯†é“º.*$', '', title)
    title = re.sub(r'\s*\|\s*æœ€æ–°èµ„è®¯.*$', '', title)
    title = re.sub(r'\s*-\s*çŸ¥è¯†é“º.*$', '', title)
    title = re.sub(r'_.*$', '', title)
    return title.strip()

def extract_title(html_path):
    """ä»HTMLæ–‡ä»¶æå–æ ‡é¢˜"""
    try:
        with open(html_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(50000)  # åªè¯»å‰50KBæé«˜é€Ÿåº¦
            parser = TitleExtractor()
            parser.feed(content)
            title = clean_title(parser.title)
            return title if title else os.path.basename(os.path.dirname(html_path))
    except Exception as e:
        return os.path.basename(os.path.dirname(html_path))

def extract_date_from_path(path_str):
    """ä»è·¯å¾„æå–æ—¥æœŸ"""
    # åŒ¹é… 20241013, 202410, 20240405 ç­‰æ ¼å¼
    match = re.search(r'/(\d{6,8})/', path_str)
    if match:
        date_str = match.group(1)
        try:
            if len(date_str) == 8:
                return datetime.strptime(date_str, '%Y%m%d').strftime('%Y-%m-%d')
            elif len(date_str) == 6:
                return datetime.strptime(date_str, '%Y%m').strftime('%Y-%m-01')
        except:
            pass
    return None

def scan_category(base_dir, category_name, icon, limit=None):
    """æ‰«æå•ä¸ªåˆ†ç±»ç›®å½•"""
    articles = []
    post_dir = Path(base_dir) / 'post'

    if not post_dir.exists():
        print(f"  âš ï¸  {category_name}: postç›®å½•ä¸å­˜åœ¨")
        return articles

    count = 0

    # éå†æ—¥æœŸç›®å½•
    for date_dir in sorted(post_dir.iterdir(), reverse=True):
        if not date_dir.is_dir() or date_dir.name == 'page':
            continue

        # éå†æ–‡ç« ç›®å½•
        for article_dir in date_dir.iterdir():
            if not article_dir.is_dir():
                continue

            index_file = article_dir / 'index.html'
            if not index_file.exists():
                continue

            # æå–ä¿¡æ¯
            title = extract_title(index_file)
            rel_path = f"/{category_name}/post/{date_dir.name}/{article_dir.name}/"
            date = extract_date_from_path(rel_path)

            articles.append({
                'title': title,
                'path': rel_path,
                'date': date or datetime.fromtimestamp(index_file.stat().st_mtime).strftime('%Y-%m-%d'),
                'category': category_name,
                'icon': icon,
                'mtime': index_file.stat().st_mtime
            })

            count += 1
            if limit and count >= limit:
                break

        if limit and count >= limit:
            break

    print(f"  âœ… {category_name}: {len(articles)} ç¯‡æ–‡ç« ")
    return articles

def generate_site_index(max_per_category=500):
    """ç”Ÿæˆå…¨ç«™ç´¢å¼•"""
    print("\nğŸš€ å¼€å§‹ç”Ÿæˆå…¨ç«™é“¾æ¥ç´¢å¼•...")
    print("=" * 60)

    # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆscriptsçš„çˆ¶ç›®å½•ï¼‰
    base_path = Path(__file__).parent.parent
    all_articles = []

    # åˆ†ç±»é…ç½®
    categories = [
        ('ai', 'AIäººå·¥æ™ºèƒ½', 'ğŸ¤–'),
        ('ai001', 'AIä¸“åŒº001', 'ğŸ¤–'),
        ('ai002', 'AIä¸“åŒº002', 'ğŸ¤–'),
        ('geek', 'æŠ€æœ¯å¼€å‘', 'ğŸ’»'),
        ('geek001', 'æŠ€æœ¯001', 'ğŸ’»'),
        ('geek002', 'æŠ€æœ¯002', 'ğŸ’»'),
        ('stock', 'è‚¡ç¥¨é‡‘è', 'ğŸ“ˆ'),
        ('stock001', 'è‚¡ç¥¨001', 'ğŸ“ˆ'),
        ('stock002', 'è‚¡ç¥¨002', 'ğŸ“ˆ'),
        ('gpt', 'GPTå¤§æ¨¡å‹', 'ğŸ§ '),
        ('go', 'Goè¯­è¨€', 'ğŸ¹'),
        ('ecg', 'å¥åº·åŒ»ç–—', 'ğŸ’Š'),
        ('ds', 'æ•°æ®ç§‘å­¦', 'ğŸ“Š'),
    ]

    for dir_name, cat_name, icon in categories:
        cat_path = base_path / dir_name
        if cat_path.exists():
            articles = scan_category(cat_path, dir_name, icon, max_per_category)
            all_articles.extend(articles)

    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    all_articles.sort(key=lambda x: x['mtime'], reverse=True)

    # ç§»é™¤mtimeå­—æ®µ
    for article in all_articles:
        del article['mtime']

    print("\n" + "=" * 60)
    print(f"ğŸ“Š æ€»è®¡: {len(all_articles)} ç¯‡æ–‡ç« ")
    print("=" * 60)

    return all_articles

def generate_json_files(articles):
    """ç”Ÿæˆå¤šä¸ªJSONæ–‡ä»¶"""
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    base_path = Path(__file__).parent.parent

    # 1. å®Œæ•´ç´¢å¼•
    full_index = {
        'generated_at': datetime.now().isoformat(),
        'total': len(articles),
        'articles': articles
    }

    with open(base_path / 'site-links-full.json', 'w', encoding='utf-8') as f:
        json.dump(full_index, f, ensure_ascii=False, indent=2)
    print("âœ… ç”Ÿæˆ: site-links-full.json")

    # 2. æœ€æ–°100ç¯‡ï¼ˆé¦–é¡µç”¨ï¼‰
    recent_100 = {
        'generated_at': datetime.now().isoformat(),
        'total': len(articles[:100]),
        'articles': articles[:100]
    }

    with open(base_path / 'site-links-recent.json', 'w', encoding='utf-8') as f:
        json.dump(recent_100, f, ensure_ascii=False, indent=2)
    print("âœ… ç”Ÿæˆ: site-links-recent.json")

    # 3. æŒ‰åˆ†ç±»åˆ†ç»„
    by_category = defaultdict(list)
    for article in articles:
        by_category[article['category']].append(article)

    category_index = {
        'generated_at': datetime.now().isoformat(),
        'categories': {cat: len(arts) for cat, arts in by_category.items()},
        'data': dict(by_category)
    }

    with open(base_path / 'site-links-by-category.json', 'w', encoding='utf-8') as f:
        json.dump(category_index, f, ensure_ascii=False, indent=2)
    print("âœ… ç”Ÿæˆ: site-links-by-category.json")

    # 4. æŒ‰å¹´æœˆåˆ†ç»„
    by_month = defaultdict(list)
    for article in articles:
        if article['date']:
            month_key = article['date'][:7]  # YYYY-MM
            by_month[month_key].append(article)

    month_index = {
        'generated_at': datetime.now().isoformat(),
        'months': sorted(by_month.keys(), reverse=True),
        'data': dict(by_month)
    }

    with open(base_path / 'site-links-by-month.json', 'w', encoding='utf-8') as f:
        json.dump(month_index, f, ensure_ascii=False, indent=2)
    print("âœ… ç”Ÿæˆ: site-links-by-month.json")

    # 5. ç²¾ç®€ç‰ˆï¼ˆåªåŒ…å«æ ‡é¢˜å’Œè·¯å¾„ï¼Œæœç´¢ç”¨ï¼‰
    search_index = [
        {'t': a['title'], 'p': a['path'], 'c': a['category']}
        for a in articles
    ]

    with open(base_path / 'site-links-search.json', 'w', encoding='utf-8') as f:
        json.dump(search_index, f, ensure_ascii=False, separators=(',', ':'))
    print("âœ… ç”Ÿæˆ: site-links-search.json (å‹ç¼©æ ¼å¼)")

def generate_stats(articles):
    """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 60)

    # æŒ‰åˆ†ç±»ç»Ÿè®¡
    by_cat = defaultdict(int)
    for a in articles:
        by_cat[a['category']] += 1

    print("\næŒ‰åˆ†ç±»:")
    for cat, count in sorted(by_cat.items(), key=lambda x: x[1], reverse=True):
        icon = next((c[2] for c in [
            ('ai', 'AIäººå·¥æ™ºèƒ½', 'ğŸ¤–'), ('geek', 'æŠ€æœ¯å¼€å‘', 'ğŸ’»'),
            ('stock', 'è‚¡ç¥¨é‡‘è', 'ğŸ“ˆ'), ('gpt', 'GPT', 'ğŸ§ '),
            ('go', 'Go', 'ğŸ¹'), ('ecg', 'å¥åº·', 'ğŸ’Š')
        ] if cat.startswith(c[0])), 'ğŸ“„')
        print(f"  {icon} {cat:12s}: {count:4d} ç¯‡")

    # æŒ‰å¹´ä»½ç»Ÿè®¡
    by_year = defaultdict(int)
    for a in articles:
        if a['date']:
            year = a['date'][:4]
            by_year[year] += 1

    print("\næŒ‰å¹´ä»½:")
    for year, count in sorted(by_year.items(), reverse=True):
        print(f"  ğŸ“… {year}: {count:4d} ç¯‡")

    print("\n" + "=" * 60)

def generate_sitemaps(articles, base_url='https://index.zshipu.com'):
    """ç”Ÿæˆå®Œæ•´çš„sitemap.xmlæ–‡ä»¶é›†"""
    print("\nğŸ—ºï¸  ç”ŸæˆSitemap XMLæ–‡ä»¶...")
    print("-" * 60)

    base_path = Path(__file__).parent.parent

    # æŒ‰åˆ†ç±»åˆ†ç»„æ–‡ç« 
    by_category = defaultdict(list)
    for article in articles:
        by_category[article['category']].append(article)

    # 1. ç”Ÿæˆæ ¸å¿ƒé¡µé¢sitemap
    core_pages = [
        ('', 1.0, 'daily'),
        ('/ai/', 0.9, 'daily'),
        ('/geek/', 0.9, 'daily'),
        ('/stock/', 0.9, 'daily'),
        ('/gpt/', 0.8, 'weekly'),
        ('/go/', 0.8, 'weekly'),
        ('/ecg/', 0.8, 'weekly'),
        ('/archives/', 0.7, 'weekly'),
        ('/categories/', 0.7, 'weekly'),
        ('/tags/', 0.7, 'weekly'),
        ('/about/', 0.6, 'monthly'),
        ('/search/', 0.5, 'monthly'),
        ('/ai1000website/', 0.9, 'weekly'),
    ]

    sitemap_pages = ['<?xml version="1.0" encoding="UTF-8"?>']
    sitemap_pages.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')

    for path, priority, changefreq in core_pages:
        sitemap_pages.append('  <url>')
        sitemap_pages.append(f'    <loc>{base_url}{path}</loc>')
        sitemap_pages.append(f'    <lastmod>{datetime.now().strftime("%Y-%m-%d")}</lastmod>')
        sitemap_pages.append(f'    <changefreq>{changefreq}</changefreq>')
        sitemap_pages.append(f'    <priority>{priority}</priority>')
        sitemap_pages.append('  </url>')

    sitemap_pages.append('</urlset>')

    with open(base_path / 'sitemap-pages.xml', 'w', encoding='utf-8') as f:
        f.write('\n'.join(sitemap_pages))
    print("âœ… sitemap-pages.xml (æ ¸å¿ƒé¡µé¢)")

    # 2. ä¸ºä¸»è¦åˆ†ç±»ç”Ÿæˆæ–‡ç« sitemap
    category_sitemaps = []

    for cat_prefix in ['ai', 'geek', 'stock']:
        # åˆå¹¶æ‰€æœ‰ç›¸å…³åˆ†ç±»çš„æ–‡ç« 
        cat_articles = []
        for cat_name, articles_list in by_category.items():
            if cat_name.startswith(cat_prefix):
                cat_articles.extend(articles_list)

        if not cat_articles:
            continue

        sitemap_name = f'sitemap-posts-{cat_prefix}.xml'
        category_sitemaps.append(sitemap_name)

        sitemap_lines = ['<?xml version="1.0" encoding="UTF-8"?>']
        sitemap_lines.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')

        for article in cat_articles[:1000]:  # é™åˆ¶æ¯ä¸ªsitemapæœ€å¤š1000ä¸ªURL
            # æ ¹æ®æ–‡ç« å‘å¸ƒæ—¶é—´è®¡ç®—ä¼˜å…ˆçº§
            priority = 0.8
            if article.get('date'):
                try:
                    article_date = datetime.strptime(article['date'], '%Y-%m-%d')
                    days_old = (datetime.now() - article_date).days
                    if days_old < 30:
                        priority = 0.9
                    elif days_old < 90:
                        priority = 0.8
                    elif days_old < 365:
                        priority = 0.7
                    else:
                        priority = 0.6
                except:
                    pass

            sitemap_lines.append('  <url>')
            sitemap_lines.append(f'    <loc>{base_url}{article["path"]}</loc>')
            if article.get('date'):
                sitemap_lines.append(f'    <lastmod>{article["date"]}</lastmod>')
            sitemap_lines.append('    <changefreq>monthly</changefreq>')
            sitemap_lines.append(f'    <priority>{priority}</priority>')
            sitemap_lines.append('  </url>')

        sitemap_lines.append('</urlset>')

        with open(base_path / sitemap_name, 'w', encoding='utf-8') as f:
            f.write('\n'.join(sitemap_lines))
        print(f"âœ… {sitemap_name} ({len(cat_articles[:1000])} ç¯‡æ–‡ç« )")

    # 3. ç”Ÿæˆå…¶ä»–åˆ†ç±»æ–‡ç« sitemap (gpt, go, ecg, ds)
    other_articles = []
    for cat_name in ['gpt', 'go', 'ecg', 'ds']:
        if cat_name in by_category:
            other_articles.extend(by_category[cat_name])

    if other_articles:
        sitemap_name = 'sitemap-posts-other.xml'
        category_sitemaps.append(sitemap_name)

        sitemap_lines = ['<?xml version="1.0" encoding="UTF-8"?>']
        sitemap_lines.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')

        for article in other_articles:
            sitemap_lines.append('  <url>')
            sitemap_lines.append(f'    <loc>{base_url}{article["path"]}</loc>')
            if article.get('date'):
                sitemap_lines.append(f'    <lastmod>{article["date"]}</lastmod>')
            sitemap_lines.append('    <changefreq>monthly</changefreq>')
            sitemap_lines.append('    <priority>0.7</priority>')
            sitemap_lines.append('  </url>')

        sitemap_lines.append('</urlset>')

        with open(base_path / sitemap_name, 'w', encoding='utf-8') as f:
            f.write('\n'.join(sitemap_lines))
        print(f"âœ… {sitemap_name} ({len(other_articles)} ç¯‡æ–‡ç« )")

    # 4. ç”Ÿæˆåˆ†ç±»é¡µé¢sitemap
    categories_to_list = ['ai', 'ai001', 'ai002', 'geek', 'geek001', 'geek002',
                          'stock', 'stock001', 'stock002', 'gpt', 'go', 'ecg', 'ds']

    sitemap_cats = ['<?xml version="1.0" encoding="UTF-8"?>']
    sitemap_cats.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')

    for cat in categories_to_list:
        sitemap_cats.append('  <url>')
        sitemap_cats.append(f'    <loc>{base_url}/{cat}/</loc>')
        sitemap_cats.append(f'    <lastmod>{datetime.now().strftime("%Y-%m-%d")}</lastmod>')
        sitemap_cats.append('    <changefreq>weekly</changefreq>')
        sitemap_cats.append('    <priority>0.8</priority>')
        sitemap_cats.append('  </url>')

    sitemap_cats.append('</urlset>')

    with open(base_path / 'sitemap-categories.xml', 'w', encoding='utf-8') as f:
        f.write('\n'.join(sitemap_cats))
    print(f"âœ… sitemap-categories.xml ({len(categories_to_list)} ä¸ªåˆ†ç±»)")

    # 5. ç”Ÿæˆæ ‡ç­¾sitemap (å ä½ç¬¦ï¼Œå®é™…æ ‡ç­¾éœ€è¦ä»æ–‡ç« ä¸­æå–)
    sitemap_tags = ['<?xml version="1.0" encoding="UTF-8"?>']
    sitemap_tags.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')

    # æ·»åŠ ä¸»è¦æ ‡ç­¾é¡µé¢
    common_tags = ['AI', 'ChatGPT', 'Python', 'Java', 'Go', 'è‚¡ç¥¨', 'æŠ•èµ„', 'æŠ€æœ¯',
                   'ç¼–ç¨‹', 'æ•°æ®ç§‘å­¦', 'ECG', 'å¥åº·', 'GPT', 'AIå·¥å…·']

    for tag in common_tags:
        sitemap_tags.append('  <url>')
        sitemap_tags.append(f'    <loc>{base_url}/tags/{tag}/</loc>')
        sitemap_tags.append(f'    <lastmod>{datetime.now().strftime("%Y-%m-%d")}</lastmod>')
        sitemap_tags.append('    <changefreq>weekly</changefreq>')
        sitemap_tags.append('    <priority>0.6</priority>')
        sitemap_tags.append('  </url>')

    sitemap_tags.append('</urlset>')

    with open(base_path / 'sitemap-tags.xml', 'w', encoding='utf-8') as f:
        f.write('\n'.join(sitemap_tags))
    print(f"âœ… sitemap-tags.xml ({len(common_tags)} ä¸ªæ ‡ç­¾)")

    # 6. ç”Ÿæˆä¸»sitemapç´¢å¼•æ–‡ä»¶
    sitemap_index = ['<?xml version="1.0" encoding="UTF-8"?>']
    sitemap_index.append('<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')

    all_sitemaps = ['sitemap-pages.xml'] + category_sitemaps + ['sitemap-categories.xml', 'sitemap-tags.xml']

    for sitemap_file in all_sitemaps:
        sitemap_index.append('  <sitemap>')
        sitemap_index.append(f'    <loc>{base_url}/{sitemap_file}</loc>')
        sitemap_index.append(f'    <lastmod>{datetime.now().strftime("%Y-%m-%d")}</lastmod>')
        sitemap_index.append('  </sitemap>')

    sitemap_index.append('</sitemapindex>')

    with open(base_path / 'sitemap.xml', 'w', encoding='utf-8') as f:
        f.write('\n'.join(sitemap_index))
    print(f"âœ… sitemap.xml (ä¸»ç´¢å¼•, åŒ…å« {len(all_sitemaps)} ä¸ªå­sitemap)")

    print("\nğŸ“Š Sitemapç”Ÿæˆç»Ÿè®¡:")
    print(f"  - æ ¸å¿ƒé¡µé¢: 13 ä¸ª")
    print(f"  - æ–‡ç« é¡µé¢: {len(articles)} ç¯‡")
    print(f"  - åˆ†ç±»é¡µé¢: {len(categories_to_list)} ä¸ª")
    print(f"  - æ ‡ç­¾é¡µé¢: {len(common_tags)} ä¸ª")
    print(f"  - æ€»è®¡URL: {13 + len(articles) + len(categories_to_list) + len(common_tags)} ä¸ª")

def generate_robots_txt():
    """ç”Ÿæˆrobots.txtæ–‡ä»¶"""
    print("\nğŸ¤– ç”Ÿæˆrobots.txtæ–‡ä»¶...")
    print("-" * 60)

    base_path = Path(__file__).parent.parent

    robots_content = """# robots.txt for index.zshipu.com

User-agent: *
Allow: /

# ç¦æ­¢ç´¢å¼•çš„ç›®å½•
Disallow: /admin/
Disallow: /private/
Disallow: /temp/
Disallow: /scripts/

# ç¦æ­¢ç´¢å¼•åˆ†é¡µé¡µé¢ï¼ˆé¿å…é‡å¤å†…å®¹ï¼‰
Disallow: /page/
Disallow: /*/page/

# å…è®¸CSSå’ŒJSï¼ˆåˆ©äºæ¸²æŸ“ï¼‰
Allow: /css/
Allow: /js/
Allow: /images/

# Sitemapä½ç½®
Sitemap: https://index.zshipu.com/sitemap.xml

# çˆ¬å–å»¶è¿Ÿï¼ˆå¯é€‰ï¼‰
Crawl-delay: 1
"""

    with open(base_path / 'robots.txt', 'w', encoding='utf-8') as f:
        f.write(robots_content.strip())

    print("âœ… robots.txt (æœç´¢å¼•æ“æŒ‡å¼•æ–‡ä»¶)")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("ğŸš€ çŸ¥è¯†é“º - å…¨ç«™é“¾æ¥ç´¢å¼•ç”Ÿæˆå™¨")
    print("   ULTRATHINK å†…é“¾ä¼˜åŒ–ç³»ç»Ÿ")
    print("=" * 60)

    # ç”Ÿæˆç´¢å¼•
    articles = generate_site_index(max_per_category=500)

    if not articles:
        print("\nâŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ")
        return

    # ç”ŸæˆJSONæ–‡ä»¶
    print("\nğŸ“ ç”ŸæˆJSONæ–‡ä»¶...")
    print("-" * 60)
    generate_json_files(articles)

    # ç”ŸæˆSitemapæ–‡ä»¶
    generate_sitemaps(articles)

    # ç”Ÿæˆrobots.txt
    generate_robots_txt()

    # ç»Ÿè®¡ä¿¡æ¯
    generate_stats(articles)

    # æ˜¾ç¤ºç¤ºä¾‹
    print("\nğŸ“° æœ€æ–°10ç¯‡æ–‡ç« :")
    print("-" * 60)
    for i, article in enumerate(articles[:10], 1):
        print(f"{i:2d}. [{article['icon']} {article['category']:8s}] {article['title'][:50]}...")

    print("\n" + "=" * 60)
    print("âœ… å…¨éƒ¨å®Œæˆï¼")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  ğŸ“„ JSONæ•°æ®æ–‡ä»¶:")
    print("     - site-links-full.json        (å®Œæ•´ç´¢å¼•)")
    print("     - site-links-recent.json      (æœ€æ–°100ç¯‡ï¼Œé¦–é¡µç”¨)")
    print("     - site-links-by-category.json (æŒ‰åˆ†ç±»)")
    print("     - site-links-by-month.json    (æŒ‰æœˆä»½)")
    print("     - site-links-search.json      (æœç´¢ç”¨ï¼Œå‹ç¼©æ ¼å¼)")
    print("\n  ğŸ—ºï¸  SEO Sitemapæ–‡ä»¶:")
    print("     - sitemap.xml                 (ä¸»ç´¢å¼•)")
    print("     - sitemap-pages.xml           (æ ¸å¿ƒé¡µé¢)")
    print("     - sitemap-posts-ai.xml        (AIåˆ†ç±»æ–‡ç« )")
    print("     - sitemap-posts-geek.xml      (æŠ€æœ¯åˆ†ç±»æ–‡ç« )")
    print("     - sitemap-posts-stock.xml     (è‚¡ç¥¨åˆ†ç±»æ–‡ç« )")
    print("     - sitemap-posts-other.xml     (å…¶ä»–åˆ†ç±»æ–‡ç« )")
    print("     - sitemap-categories.xml      (åˆ†ç±»é¡µé¢)")
    print("     - sitemap-tags.xml            (æ ‡ç­¾é¡µé¢)")
    print("\n  ğŸ¤– SEOé…ç½®æ–‡ä»¶:")
    print("     - robots.txt                  (æœç´¢å¼•æ“æŒ‡å¼•)")
    print("=" * 60 + "\n")

if __name__ == '__main__':
    main()
