#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†é“ºå…¨ç«™é“¾æ¥ç´¢å¼•ç”Ÿæˆå™¨ - ULTRATHINKå†…é“¾ä¼˜åŒ–
ç”ŸæˆJSONæ ¼å¼çš„å…¨ç«™æ–‡ç« ç´¢å¼•ï¼Œç”¨äºï¼š
1. Archivesé¡µé¢ç«™ç‚¹åœ°å›¾
2. é¦–é¡µå³ä¾§æ å†…é“¾å¡ç‰‡
3. å…¨ç«™æœç´¢åŠŸèƒ½
"""

import os
import sys
import io
import json
import re
from pathlib import Path
from datetime import datetime
from html.parser import HTMLParser
from collections import defaultdict

# ä¿®å¤Windowsæ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

class TitleExtractor(HTMLParser):
    """HTMLæ ‡é¢˜æå–å™¨"""
    def __init__(self):
        super().__init__()
        self.in_title = False
        self.title = ""

    def handle_starttag(self, tag, attrs):
        if tag == "title":
            self.in_title = True

    def handle_data(self, data):
        if self.in_title:
            self.title += data

    def handle_endtag(self, tag):
        if tag == "title":
            self.in_title = False

def clean_title(title):
    """æ¸…ç†æ ‡é¢˜"""
    # ç§»é™¤å¸¸è§åç¼€
    title = re.sub(r'--çŸ¥è¯†é“º.*$', '', title)
    title = re.sub(r'\s*\|\s*æœ€æ–°èµ„è®¯.*$', '', title)
    title = re.sub(r'\s*-\s*çŸ¥è¯†é“º.*$', '', title)
    title = re.sub(r'_.*$', '', title)
    return title.strip()

def extract_title(html_path):
    """ä»HTMLæ–‡ä»¶æå–æ ‡é¢˜"""
    try:
        with open(html_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(50000)  # åªè¯»å‰50KBæé«˜é€Ÿåº¦
            parser = TitleExtractor()
            parser.feed(content)
            title = clean_title(parser.title)
            return title if title else os.path.basename(os.path.dirname(html_path))
    except Exception as e:
        return os.path.basename(os.path.dirname(html_path))

def extract_date_from_path(path_str):
    """ä»è·¯å¾„æå–æ—¥æœŸ"""
    # åŒ¹é… 20241013, 202410, 20240405 ç­‰æ ¼å¼
    match = re.search(r'/(\d{6,8})/', path_str)
    if match:
        date_str = match.group(1)
        try:
            if len(date_str) == 8:
                return datetime.strptime(date_str, '%Y%m%d').strftime('%Y-%m-%d')
            elif len(date_str) == 6:
                return datetime.strptime(date_str, '%Y%m').strftime('%Y-%m-01')
        except:
            pass
    return None

def scan_category(base_dir, category_name, icon, limit=None):
    """æ‰«æå•ä¸ªåˆ†ç±»ç›®å½•"""
    articles = []
    post_dir = Path(base_dir) / 'post'

    if not post_dir.exists():
        print(f"  âš ï¸  {category_name}: postç›®å½•ä¸å­˜åœ¨")
        return articles

    count = 0

    # éå†æ—¥æœŸç›®å½•
    for date_dir in sorted(post_dir.iterdir(), reverse=True):
        if not date_dir.is_dir() or date_dir.name == 'page':
            continue

        # éå†æ–‡ç« ç›®å½•
        for article_dir in date_dir.iterdir():
            if not article_dir.is_dir():
                continue

            index_file = article_dir / 'index.html'
            if not index_file.exists():
                continue

            # æå–ä¿¡æ¯
            title = extract_title(index_file)
            rel_path = f"/{category_name}/post/{date_dir.name}/{article_dir.name}/"
            date = extract_date_from_path(rel_path)

            articles.append({
                'title': title,
                'path': rel_path,
                'date': date or datetime.fromtimestamp(index_file.stat().st_mtime).strftime('%Y-%m-%d'),
                'category': category_name,
                'icon': icon,
                'mtime': index_file.stat().st_mtime
            })

            count += 1
            if limit and count >= limit:
                break

        if limit and count >= limit:
            break

    print(f"  âœ… {category_name}: {len(articles)} ç¯‡æ–‡ç« ")
    return articles

def generate_site_index(max_per_category=500):
    """ç”Ÿæˆå…¨ç«™ç´¢å¼•"""
    print("\nğŸš€ å¼€å§‹ç”Ÿæˆå…¨ç«™é“¾æ¥ç´¢å¼•...")
    print("=" * 60)

    # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆscriptsçš„çˆ¶ç›®å½•ï¼‰
    base_path = Path(__file__).parent.parent
    all_articles = []

    # åˆ†ç±»é…ç½®
    categories = [
        ('ai', 'AIäººå·¥æ™ºèƒ½', 'ğŸ¤–'),
        ('ai001', 'AIä¸“åŒº001', 'ğŸ¤–'),
        ('ai002', 'AIä¸“åŒº002', 'ğŸ¤–'),
        ('geek', 'æŠ€æœ¯å¼€å‘', 'ğŸ’»'),
        ('geek001', 'æŠ€æœ¯001', 'ğŸ’»'),
        ('geek002', 'æŠ€æœ¯002', 'ğŸ’»'),
        ('stock', 'è‚¡ç¥¨é‡‘è', 'ğŸ“ˆ'),
        ('stock001', 'è‚¡ç¥¨001', 'ğŸ“ˆ'),
        ('stock002', 'è‚¡ç¥¨002', 'ğŸ“ˆ'),
        ('gpt', 'GPTå¤§æ¨¡å‹', 'ğŸ§ '),
        ('go', 'Goè¯­è¨€', 'ğŸ¹'),
        ('ecg', 'å¥åº·åŒ»ç–—', 'ğŸ’Š'),
        ('ds', 'æ•°æ®ç§‘å­¦', 'ğŸ“Š'),
    ]

    for dir_name, cat_name, icon in categories:
        cat_path = base_path / dir_name
        if cat_path.exists():
            articles = scan_category(cat_path, dir_name, icon, max_per_category)
            all_articles.extend(articles)

    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    all_articles.sort(key=lambda x: x['mtime'], reverse=True)

    # ç§»é™¤mtimeå­—æ®µ
    for article in all_articles:
        del article['mtime']

    print("\n" + "=" * 60)
    print(f"ğŸ“Š æ€»è®¡: {len(all_articles)} ç¯‡æ–‡ç« ")
    print("=" * 60)

    return all_articles

def generate_json_files(articles):
    """ç”Ÿæˆå¤šä¸ªJSONæ–‡ä»¶"""
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    base_path = Path(__file__).parent.parent

    # 1. å®Œæ•´ç´¢å¼•
    full_index = {
        'generated_at': datetime.now().isoformat(),
        'total': len(articles),
        'articles': articles
    }

    with open(base_path / 'site-links-full.json', 'w', encoding='utf-8') as f:
        json.dump(full_index, f, ensure_ascii=False, indent=2)
    print("âœ… ç”Ÿæˆ: site-links-full.json")

    # 2. æœ€æ–°100ç¯‡ï¼ˆé¦–é¡µç”¨ï¼‰
    recent_100 = {
        'generated_at': datetime.now().isoformat(),
        'total': len(articles[:100]),
        'articles': articles[:100]
    }

    with open(base_path / 'site-links-recent.json', 'w', encoding='utf-8') as f:
        json.dump(recent_100, f, ensure_ascii=False, indent=2)
    print("âœ… ç”Ÿæˆ: site-links-recent.json")

    # 3. æŒ‰åˆ†ç±»åˆ†ç»„
    by_category = defaultdict(list)
    for article in articles:
        by_category[article['category']].append(article)

    category_index = {
        'generated_at': datetime.now().isoformat(),
        'categories': {cat: len(arts) for cat, arts in by_category.items()},
        'data': dict(by_category)
    }

    with open(base_path / 'site-links-by-category.json', 'w', encoding='utf-8') as f:
        json.dump(category_index, f, ensure_ascii=False, indent=2)
    print("âœ… ç”Ÿæˆ: site-links-by-category.json")

    # 4. æŒ‰å¹´æœˆåˆ†ç»„
    by_month = defaultdict(list)
    for article in articles:
        if article['date']:
            month_key = article['date'][:7]  # YYYY-MM
            by_month[month_key].append(article)

    month_index = {
        'generated_at': datetime.now().isoformat(),
        'months': sorted(by_month.keys(), reverse=True),
        'data': dict(by_month)
    }

    with open(base_path / 'site-links-by-month.json', 'w', encoding='utf-8') as f:
        json.dump(month_index, f, ensure_ascii=False, indent=2)
    print("âœ… ç”Ÿæˆ: site-links-by-month.json")

    # 5. ç²¾ç®€ç‰ˆï¼ˆåªåŒ…å«æ ‡é¢˜å’Œè·¯å¾„ï¼Œæœç´¢ç”¨ï¼‰
    search_index = [
        {'t': a['title'], 'p': a['path'], 'c': a['category']}
        for a in articles
    ]

    with open(base_path / 'site-links-search.json', 'w', encoding='utf-8') as f:
        json.dump(search_index, f, ensure_ascii=False, separators=(',', ':'))
    print("âœ… ç”Ÿæˆ: site-links-search.json (å‹ç¼©æ ¼å¼)")

def generate_stats(articles):
    """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 60)

    # æŒ‰åˆ†ç±»ç»Ÿè®¡
    by_cat = defaultdict(int)
    for a in articles:
        by_cat[a['category']] += 1

    print("\næŒ‰åˆ†ç±»:")
    for cat, count in sorted(by_cat.items(), key=lambda x: x[1], reverse=True):
        icon = next((c[2] for c in [
            ('ai', 'AIäººå·¥æ™ºèƒ½', 'ğŸ¤–'), ('geek', 'æŠ€æœ¯å¼€å‘', 'ğŸ’»'),
            ('stock', 'è‚¡ç¥¨é‡‘è', 'ğŸ“ˆ'), ('gpt', 'GPT', 'ğŸ§ '),
            ('go', 'Go', 'ğŸ¹'), ('ecg', 'å¥åº·', 'ğŸ’Š')
        ] if cat.startswith(c[0])), 'ğŸ“„')
        print(f"  {icon} {cat:12s}: {count:4d} ç¯‡")

    # æŒ‰å¹´ä»½ç»Ÿè®¡
    by_year = defaultdict(int)
    for a in articles:
        if a['date']:
            year = a['date'][:4]
            by_year[year] += 1

    print("\næŒ‰å¹´ä»½:")
    for year, count in sorted(by_year.items(), reverse=True):
        print(f"  ğŸ“… {year}: {count:4d} ç¯‡")

    print("\n" + "=" * 60)

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("ğŸš€ çŸ¥è¯†é“º - å…¨ç«™é“¾æ¥ç´¢å¼•ç”Ÿæˆå™¨")
    print("   ULTRATHINK å†…é“¾ä¼˜åŒ–ç³»ç»Ÿ")
    print("=" * 60)

    # ç”Ÿæˆç´¢å¼•
    articles = generate_site_index(max_per_category=500)

    if not articles:
        print("\nâŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ")
        return

    # ç”ŸæˆJSONæ–‡ä»¶
    print("\nğŸ“ ç”ŸæˆJSONæ–‡ä»¶...")
    print("-" * 60)
    generate_json_files(articles)

    # ç»Ÿè®¡ä¿¡æ¯
    generate_stats(articles)

    # æ˜¾ç¤ºç¤ºä¾‹
    print("\nğŸ“° æœ€æ–°10ç¯‡æ–‡ç« :")
    print("-" * 60)
    for i, article in enumerate(articles[:10], 1):
        print(f"{i:2d}. [{article['icon']} {article['category']:8s}] {article['title'][:50]}...")

    print("\n" + "=" * 60)
    print("âœ… å…¨éƒ¨å®Œæˆï¼")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - site-links-full.json        (å®Œæ•´ç´¢å¼•)")
    print("  - site-links-recent.json      (æœ€æ–°100ç¯‡ï¼Œé¦–é¡µç”¨)")
    print("  - site-links-by-category.json (æŒ‰åˆ†ç±»)")
    print("  - site-links-by-month.json    (æŒ‰æœˆä»½)")
    print("  - site-links-search.json      (æœç´¢ç”¨ï¼Œå‹ç¼©æ ¼å¼)")
    print("=" * 60 + "\n")

if __name__ == '__main__':
    main()
