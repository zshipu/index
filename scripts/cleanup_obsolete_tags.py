#!/usr/bin/env python3
"""
æ¸…ç† JSON æ–‡ä»¶ä¸­æŒ‡å‘åºŸå¼ƒæ ‡ç­¾ç›®å½•çš„æ¡ç›®
Remove entries pointing to obsolete /tags/<tag-name>/ directories
"""

import json
import os
from datetime import datetime

# å·²åˆ é™¤çš„åºŸå¼ƒæ ‡ç­¾ç›®å½•åˆ—è¡¨
OBSOLETE_TAG_DIRS = [
    "AIæµªæ½®",
    "ChatGPT",
    "OpenAI",
    "Prompt-Engineering",
    "PromptæŠ€å·§",
    "PromptæŠ€æœ¯",
    "è¡¨è¾¾èƒ½åŠ›",
    "å¤§æ¨¡å‹åä½œ",
    "æŠ€å·§æ¡†æ¶",
    "äººå·¥æ™ºèƒ½",
    "è‡ªç„¶è¯­è¨€å¤„ç†"
]

def cleanup_site_tags_enhanced(file_path):
    """æ¸…ç† site-tags-enhanced.json"""
    print(f"\n[*] Processing: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    original_count = len(data.get('tags', []))
    print(f"   Original tags: {original_count}")

    # è¿‡æ»¤æ‰åºŸå¼ƒçš„æ ‡ç­¾
    cleaned_tags = []
    removed_tags = []

    for tag in data.get('tags', []):
        tag_name = tag.get('name', '')
        tag_url = tag.get('url', '')

        # æ£€æŸ¥æ˜¯å¦æ˜¯åºŸå¼ƒçš„æ ‡ç­¾ç›®å½•
        is_obsolete = False
        for obsolete_name in OBSOLETE_TAG_DIRS:
            if tag_name == obsolete_name or f"/tags/{obsolete_name}/" in tag_url:
                is_obsolete = True
                removed_tags.append(tag_name)
                break

        if not is_obsolete:
            cleaned_tags.append(tag)

    # æ›´æ–°æ•°æ®
    data['tags'] = cleaned_tags
    data['total_tags'] = len(cleaned_tags)
    data['generated_at'] = datetime.now().isoformat()

    # å¤‡ä»½åŸæ–‡ä»¶
    backup_path = file_path + '.backup-obsolete-cleanup'
    if not os.path.exists(backup_path):
        with open(backup_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"   âœ… Backup created: {backup_path}")

    # å†™å…¥æ¸…ç†åçš„æ•°æ®
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"   âœ… Cleaned tags: {len(cleaned_tags)}")
    print(f"   âŒ Removed: {len(removed_tags)} tags")
    if removed_tags:
        print(f"   Removed tags: {', '.join(removed_tags)}")

    return len(removed_tags)

def cleanup_site_tags(file_path):
    """æ¸…ç† site-tags.json"""
    print(f"\nğŸ“„ Processing: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    original_count = len(data.get('tags', []))
    print(f"   Original tags: {original_count}")

    # è¿‡æ»¤æ‰åºŸå¼ƒçš„æ ‡ç­¾
    cleaned_tags = []
    removed_tags = []

    for tag in data.get('tags', []):
        tag_name = tag.get('name', '')

        if tag_name not in OBSOLETE_TAG_DIRS:
            cleaned_tags.append(tag)
        else:
            removed_tags.append(tag_name)

    # æ›´æ–°æ•°æ®
    data['tags'] = cleaned_tags

    # å¤‡ä»½
    backup_path = file_path + '.backup-obsolete-cleanup'
    if not os.path.exists(backup_path):
        import shutil
        shutil.copy2(file_path, backup_path)
        print(f"   âœ… Backup created: {backup_path}")

    # å†™å…¥
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"   âœ… Cleaned tags: {len(cleaned_tags)}")
    print(f"   âŒ Removed: {len(removed_tags)} tags")
    if removed_tags:
        print(f"   Removed tags: {', '.join(removed_tags)}")

    return len(removed_tags)

def cleanup_tag_hot(file_path):
    """æ¸…ç† tag-hot.json"""
    print(f"\nğŸ“„ Processing: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    original_count = len(data.get('tags', []))
    print(f"   Original tags: {original_count}")

    # è¿‡æ»¤
    cleaned_tags = []
    removed_tags = []

    for tag in data.get('tags', []):
        tag_name = tag.get('name', '')
        tag_url = tag.get('url', '')

        is_obsolete = False
        for obsolete_name in OBSOLETE_TAG_DIRS:
            if tag_name == obsolete_name or f"/tags/{obsolete_name}/" in tag_url:
                is_obsolete = True
                removed_tags.append(tag_name)
                break

        if not is_obsolete:
            cleaned_tags.append(tag)

    # æ›´æ–°
    data['tags'] = cleaned_tags

    # å¤‡ä»½
    backup_path = file_path + '.backup-obsolete-cleanup'
    if not os.path.exists(backup_path):
        import shutil
        shutil.copy2(file_path, backup_path)
        print(f"   âœ… Backup created: {backup_path}")

    # å†™å…¥
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"   âœ… Cleaned tags: {len(cleaned_tags)}")
    print(f"   âŒ Removed: {len(removed_tags)} tags")
    if removed_tags:
        print(f"   Removed tags: {', '.join(removed_tags)}")

    return len(removed_tags)

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§¹ Cleaning Obsolete Tag Directory References from JSON Files")
    print("=" * 60)

    print(f"\nğŸ—‘ï¸  Obsolete tag directories to remove ({len(OBSOLETE_TAG_DIRS)}):")
    for tag in OBSOLETE_TAG_DIRS:
        print(f"   - {tag}")

    total_removed = 0

    # æ¸…ç†å„ä¸ª JSON æ–‡ä»¶
    files_to_clean = [
        ('site-tags-enhanced.json', cleanup_site_tags_enhanced),
        ('site-tags.json', cleanup_site_tags),
        ('tag-hot.json', cleanup_tag_hot)
    ]

    for filename, cleanup_func in files_to_clean:
        file_path = filename
        if os.path.exists(file_path):
            try:
                removed = cleanup_func(file_path)
                total_removed += removed
            except Exception as e:
                print(f"   âŒ Error processing {filename}: {e}")
        else:
            print(f"\nğŸ“„ {filename} - Not found, skipping")

    print("\n" + "=" * 60)
    print(f"âœ… Cleanup Complete!")
    print(f"   Total entries removed: {total_removed}")
    print(f"   Backups created with .backup-obsolete-cleanup extension")
    print("=" * 60)

if __name__ == '__main__':
    main()
