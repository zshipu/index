<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dify on 知识铺的博客</title>
    <link>https://index.zshipu.com/geek002/tags/Dify/</link>
    <description>Recent content in Dify on 知识铺的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 11 Nov 2024 06:02:51 +0000</lastBuildDate>
    <atom:link href="https://index.zshipu.com/geek002/tags/Dify/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Dify与Xinference GPU环境部署全流程 -- 知识铺</title>
      <link>https://index.zshipu.com/geek002/post/202411/Dify%E4%B8%8EXinference-GPU%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%85%A8%E6%B5%81%E7%A8%8B--%E7%9F%A5%E8%AF%86%E9%93%BA/</link>
      <pubDate>Mon, 11 Nov 2024 06:02:51 +0000</pubDate>
      <guid>https://index.zshipu.com/geek002/post/202411/Dify%E4%B8%8EXinference-GPU%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E5%85%A8%E6%B5%81%E7%A8%8B--%E7%9F%A5%E8%AF%86%E9%93%BA/</guid>
      <description>在本地 GPU 上部署 Dify 服务 在之前的文章《RAG 项目对比》中，我们探讨了多个RAG框架，并最终确定Dify作为最适合的选项。接下来，本文将详细介绍如何在本地GPU设备上部署Dify服务。 选择模型加载框架 Dify的设计理念之一是将模型的加载过程独立出来，这意味着我们需要选择一个合适的模型加</description>
    </item>
    <item>
      <title>折腾杂谈：运用Dify&#43;xinference&#43;ollama构建知识库 -- 知识铺</title>
      <link>https://index.zshipu.com/geek002/post/202410/%E6%8A%98%E8%85%BE%E6%9D%82%E8%B0%88%E8%BF%90%E7%94%A8Dify&#43;xinference&#43;ollama%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93--%E7%9F%A5%E8%AF%86%E9%93%BA/</link>
      <pubDate>Tue, 29 Oct 2024 07:56:06 +0000</pubDate>
      <guid>https://index.zshipu.com/geek002/post/202410/%E6%8A%98%E8%85%BE%E6%9D%82%E8%B0%88%E8%BF%90%E7%94%A8Dify&#43;xinference&#43;ollama%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93--%E7%9F%A5%E8%AF%86%E9%93%BA/</guid>
      <description>为了在本地运行一个带有重排序（Rerank）步骤的知识库问答系统，并确保该系统能在NVIDIA GeForce RTX 3060M这样的GPU上高效运作，我们可以结合使用Dify、xinference以及ollama。以下是部署此系统的步骤概述： 部署概览 不含重排序的简易配置如果不需要利用重排序来提升检</description>
    </item>
    <item>
      <title>运用Dify、Xinference与Ollama构建知识库 -- 知识铺</title>
      <link>https://index.zshipu.com/geek002/post/202410/%E8%BF%90%E7%94%A8DifyXinference%E4%B8%8EOllama%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93--%E7%9F%A5%E8%AF%86%E9%93%BA/</link>
      <pubDate>Tue, 29 Oct 2024 07:55:06 +0000</pubDate>
      <guid>https://index.zshipu.com/geek002/post/202410/%E8%BF%90%E7%94%A8DifyXinference%E4%B8%8EOllama%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93--%E7%9F%A5%E8%AF%86%E9%93%BA/</guid>
      <description>记录下运用Dify + xinference + ollama打造带重排序(Rerank)步骤的知识库问答，更好的是&amp;mdash;-即使在我的3060M上其也能完全本地运行并有不错的效果！3060M本地运行llama3-9B的生成速度参照前文。 git clone https://github.com/langgenius/dify.git cd dify/docker docker compose up -d 随后访问本地的http://localhos</description>
    </item>
    <item>
      <title>安装Dify并集成Ollama和Xinference -- 知识铺</title>
      <link>https://index.zshipu.com/geek002/post/202410/%E5%AE%89%E8%A3%85Dify%E5%B9%B6%E9%9B%86%E6%88%90Ollama%E5%92%8CXinference--%E7%9F%A5%E8%AF%86%E9%93%BA/</link>
      <pubDate>Tue, 29 Oct 2024 07:53:06 +0000</pubDate>
      <guid>https://index.zshipu.com/geek002/post/202410/%E5%AE%89%E8%A3%85Dify%E5%B9%B6%E9%9B%86%E6%88%90Ollama%E5%92%8CXinference--%E7%9F%A5%E8%AF%86%E9%93%BA/</guid>
      <description>liuqianglong.com 本文介绍了通过Docker安装Dify，然后集成Ollama和XInference，并利用Dify快速搭建一个基于知识库问答的应用。 一、Dify简介 Dify是一款开源的大语言模型（LLM）应用开发平台，旨在帮助开发者快速构建和部署生成式AI应用。以下是Dify的主要功能和特点[</description>
    </item>
    <item>
      <title>彻底搞懂大模型 - Dify（Agent &#43; RAG） -- 知识铺</title>
      <link>https://index.zshipu.com/geek002/post/202410/%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E5%A4%A7%E6%A8%A1%E5%9E%8B-DifyAgent-&#43;-RAG--%E7%9F%A5%E8%AF%86%E9%93%BA/</link>
      <pubDate>Tue, 29 Oct 2024 07:47:06 +0000</pubDate>
      <guid>https://index.zshipu.com/geek002/post/202410/%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E5%A4%A7%E6%A8%A1%E5%9E%8B-DifyAgent-&#43;-RAG--%E7%9F%A5%E8%AF%86%E9%93%BA/</guid>
      <description>aaaaaaaDify 是一个专为构建AI应用程序而设计的开源平台，它结合了后端即服务（BaaS）与LLMOps的理念。该平台支持多种大型语言模型，包括但不限于Claude3和OpenAI系列，并与多家模型供应商保持合作，确保开发者能够根据具体需求选择最合适的模型。 aaaaaaaDify 提供了一系列工具和服务来简化AI应</description>
    </item>
    <item>
      <title>Milvus与Dify结合快速构建RAG系统 -- 知识铺</title>
      <link>https://index.zshipu.com/geek002/post/202410/Milvus%E4%B8%8EDify%E7%BB%93%E5%90%88%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BARAG%E7%B3%BB%E7%BB%9F--%E7%9F%A5%E8%AF%86%E9%93%BA/</link>
      <pubDate>Tue, 29 Oct 2024 07:44:06 +0000</pubDate>
      <guid>https://index.zshipu.com/geek002/post/202410/Milvus%E4%B8%8EDify%E7%BB%93%E5%90%88%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BARAG%E7%B3%BB%E7%BB%9F--%E7%9F%A5%E8%AF%86%E9%93%BA/</guid>
      <description>近，检索增强生成（RAG）技术在 AI 界引起了广泛关注。作为一种将知识库与生成模型结合的新型架构，RAG 大大提升了 AI 应用的实际表现。而在构建 RAG 系统时，Milvus 作为业界领先的开源向量数据库，扮演着关键角色。本文将通过在 Dify 平台上使用 Milvus，带领大家构建一个高效的 RAG 系统，探索向量</description>
    </item>
    <item>
      <title>彻底搞懂大模型 - Dify（Agent &#43; RAG） -- 知识铺</title>
      <link>https://index.zshipu.com/geek002/post/20240918/%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E5%A4%A7%E6%A8%A1%E5%9E%8B-DifyAgent-&#43;-RAG--%E7%9F%A5%E8%AF%86%E9%93%BA/</link>
      <pubDate>Wed, 25 Sep 2024 03:36:09 +0000</pubDate>
      <guid>https://index.zshipu.com/geek002/post/20240918/%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E5%A4%A7%E6%A8%A1%E5%9E%8B-DifyAgent-&#43;-RAG--%E7%9F%A5%E8%AF%86%E9%93%BA/</guid>
      <description>aaaaaaaDify 是一个专为构建AI应用程序而设计的开源平台，它结合了后端即服务（Backend as a Service, BaaS）的概念与LLMOps的理念。该平台支持多种大型语言模型，包括但不限于Claude3和OpenAI的产品，并且通过与多个模型供应商合作，确保开发者能够根据其具体需求选择最合适的语言模型。</description>
    </item>
  </channel>
</rss>
